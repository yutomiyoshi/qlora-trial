{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 学習の実行\n",
    "!python qlora.py \\\n",
    "    --model_name cyberagent/calm2-7b \\\n",
    "    --output_dir \"./output/calm_peft\" \\\n",
    "    --dataset \"alpaca\" \\\n",
    "    --max_steps 1000 \\\n",
    "    --use_auth \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_strategy steps \\\n",
    "    --data_seed 42 \\\n",
    "    --save_steps 50 \\\n",
    "    --save_total_limit 40 \\\n",
    "    --max_new_tokens 32 \\\n",
    "    --dataloader_num_workers 1 \\\n",
    "    --group_by_length \\\n",
    "    --logging_strategy steps \\\n",
    "    --remove_unused_columns False \\\n",
    "    --do_train \\\n",
    "    --lora_r 64 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_modules all \\\n",
    "    --double_quant \\\n",
    "    --quant_type nf4 \\\n",
    "    --bf16 \\\n",
    "    --bits 4 \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type constant \\\n",
    "    --gradient_checkpointing \\\n",
    "    --source_max_len 16 \\\n",
    "    --target_max_len 512 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --eval_steps 187 \\\n",
    "    --learning_rate 0.0002 \\\n",
    "    --adam_beta2 0.999 \\\n",
    "    --max_grad_norm 0.3 \\\n",
    "    --lora_dropout 0.1 \\\n",
    "    --weight_decay 0.0 \\\n",
    "    --seed 0 \\\n",
    "    --load_in_4bit \\\n",
    "    --use_peft \\\n",
    "    --batch_size 4 \\\n",
    "    --gradient_accumulation_steps 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# トークナイザーとモデルの読み込み\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"cyberagent/calm2-7b-chat\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"cyberagent/calm2-7b-chat\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    ),\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "# LoRAの読み込み\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"./output/calm_peft/checkpoint-1000/adapter_model/\",\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# プロンプトの準備\n",
    "prompt = \"### Instruction: 富士山とは？\\n\\n### Response: \"\n",
    "\n",
    "# 推論の実行\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
